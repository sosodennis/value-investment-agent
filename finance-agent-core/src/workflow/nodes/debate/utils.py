"""
Utility functions for debate agent enhancements.
Includes sycophancy detection using FastEmbed and CAPM-based hurdle rate calculation.
"""

import numpy as np
from fastembed import TextEmbedding

from src.utils.logger import get_logger

from .market_data import (
    get_current_risk_free_rate,
    get_dynamic_payoff_map,
)

logger = get_logger(__name__)


class SycophancyDetector:
    """
    Detects excessive agreement between Bull and Bear agents using embeddings.
    Uses FastEmbed with sentence-transformers/all-MiniLM-L6-v2 for lightweight CPU-based similarity.
    """

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """Initialize the embedding model (cached after first download)."""
        self._embedding_model: TextEmbedding | None = None
        self.model_name = model_name

    @property
    def embedding_model(self) -> TextEmbedding:
        """Lazy load the embedding model."""
        if self._embedding_model is None:
            self._embedding_model = TextEmbedding(model_name=self.model_name)
        return self._embedding_model

    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors."""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return float(dot_product / (norm1 * norm2))

    def check_consensus(
        self, bull_thesis: str, bear_thesis: str, threshold: float = 0.8
    ) -> tuple[float, bool]:
        """
        Check if Bull and Bear theses are too similar (sycophancy).

        Args:
            bull_thesis: The Bull agent's argument
            bear_thesis: The Bear agent's argument
            threshold: Similarity threshold (default 0.75)

        Returns:
            tuple: (similarity_score, is_sycophantic)
                - similarity_score: Cosine similarity (0.0 to 1.0)
                - is_sycophantic: True if similarity > threshold
        """
        # Generate embeddings
        embeddings = list(self.embedding_model.embed([bull_thesis, bear_thesis]))

        # Extract vectors
        bull_vec = np.array(embeddings[0])
        bear_vec = np.array(embeddings[1])

        # Calculate similarity
        similarity = self.cosine_similarity(bull_vec, bear_vec)

        return similarity, similarity > threshold


# Global instance (lazy-loaded)
_detector: SycophancyDetector | None = None


def get_sycophancy_detector() -> SycophancyDetector:
    """Get or create the global sycophancy detector instance."""
    global _detector
    if _detector is None:
        _detector = SycophancyDetector()
    return _detector


def _parse_score(val) -> float:
    """Helper to parse probability scores from various formats."""
    if isinstance(val, str):
        val = val.replace("%", "").strip()
    try:
        return float(val)
    except (ValueError, TypeError):
        return 0.0


def _get_normalized_probabilities(scenarios: dict) -> tuple[float, float, float]:
    """Extract and normalize probabilities for bull, bear, and base cases."""
    s_bull = _parse_score(scenarios.get("bull_case", {}).get("probability", 0))
    s_bear = _parse_score(scenarios.get("bear_case", {}).get("probability", 0))
    s_base = _parse_score(scenarios.get("base_case", {}).get("probability", 0))

    total_score = s_bull + s_bear + s_base
    if total_score == 0:
        return 0.33, 0.33, 0.34
    return s_bull / total_score, s_bear / total_score, s_base / total_score


def _get_return_from_scenario(
    scenarios: dict, case_key: str, payoff_map: dict
) -> float:
    """Map price implication strings to numerical return values."""
    impl = scenarios.get(case_key, {}).get("price_implication", "FLAT")
    if hasattr(impl, "value"):
        impl = impl.value
    impl = str(impl).upper()

    for k, v in payoff_map.items():
        if k in impl:
            return v
    return 0.0


def calculate_pragmatic_verdict(conclusion_data: dict, ticker: str = None) -> dict:
    """
    V2.0 Simplified: The Pragmatic Reward/Risk Model

    æ ¸å¿ƒå“²å­¸ï¼š
    1. åªæœ‰å…©å€‹è®Šæ•¸é‡è¦ï¼šæ½›åœ¨ç²åˆ© (Upside) å’Œ æ½›åœ¨è™§æ (Downside)ã€‚
    2. æ³¢å‹•ä¸æ˜¯é¢¨éšªï¼Œ"æ°¸ä¹…æ€§è™§æ" æ‰æ˜¯é¢¨éšªã€‚
    3. å¦‚æœè³ ç‡ (Odds) å¤ å¥½ï¼Œæˆ‘å€‘å°±è³­ã€‚
    """
    scenarios = conclusion_data.get("scenario_analysis", {})
    risk_profile = conclusion_data.get("risk_profile", "GROWTH_TECH")

    # 1. æå–åŸºæœ¬æ•¸æ“š
    # é€™è£¡æˆ‘å€‘ä½¿ç”¨èˆ‡ä¹‹å‰ç›¸åŒçš„ normalized æ¦‚ç‡ helpers, ç¢ºä¿æ¦‚ç‡å’Œç‚º 1 (æˆ–æ¥è¿‘)
    p_bull, p_bear, p_base = _get_normalized_probabilities(scenarios)

    # ç²å–å›å ±å€¼ (ä½¿ç”¨ Payoff Mapï¼Œå‹•æ…‹æ˜ å°„ LLM çš„ price_implication)
    payoff_map = get_dynamic_payoff_map(ticker, risk_profile)
    r_bull = _get_return_from_scenario(scenarios, "bull_case", payoff_map)
    r_base = _get_return_from_scenario(scenarios, "base_case", payoff_map)
    r_bear = _get_return_from_scenario(scenarios, "bear_case", payoff_map)

    # 2. è¨ˆç®—åŠ æ¬ŠæœŸæœ›å€¼ (EV)
    # é€™æ˜¯æˆ‘å€‘çš„ "ç¾…ç›¤"ï¼Œå‘Šè¨´æˆ‘å€‘å¤§æ–¹å‘
    raw_ev = (p_bull * r_bull) + (p_base * r_base) + (p_bear * r_bear)

    # 3. è¨ˆç®— "æ©Ÿæœƒæˆæœ¬" (Alpha)
    # é€™æ˜¯å”¯ä¸€çš„ "éæ¿¾å™¨"ï¼šå¦‚æœé€£ç¾å‚µéƒ½è·‘ä¸è´ï¼Œå°±åˆ¥ç©äº†
    risk_free = get_current_risk_free_rate()
    alpha = raw_ev - risk_free

    # 4. æ ¸å¿ƒé‚è¼¯ï¼šç›ˆè™§æ¯” (Reward / Risk Ratio)
    # æˆ‘å€‘åªé—œå¿ƒï¼šçœ‹å°äº†è³ºå¤šå°‘(Upside) vs çœ‹éŒ¯äº†è³ å¤šå°‘(Downside)

    # Upside Potential (åªçœ‹æ¼²çš„æƒ…å¢ƒ)
    # é€™è£¡æˆ‘å€‘ç¨å¾®ä¿®æ”¹ä¸€ä¸‹ User çš„é‚è¼¯ï¼Œè®“ Base Case å¦‚æœæ˜¯æ­£çš„ä¹Ÿç®— Upside
    weighted_upside = (p_bull * r_bull) + (p_base * max(0, r_base))

    # Downside Risk (åªçœ‹è·Œçš„æƒ…å¢ƒï¼Œå–çµ•å°å€¼)
    # æˆ‘å€‘åŠ ä¸€é»æ¬Šé‡(1.5å€)ï¼Œä»£è¡¨æˆ‘å€‘ç¨å¾®è¨å­è³ éŒ¢ï¼Œä½†ä¸è¦åƒä¹‹å‰ Lambda é‚£éº¼èª‡å¼µ
    weighted_downside = (p_bear * abs(r_bear)) + (p_base * abs(min(0, r_base)))
    weighted_downside = weighted_downside * 1.5

    # --- æ•¸æ“šè³ªé‡æª¢æŸ¥ (Data Quality Gate) ---
    # å¦‚æœ downside æ¥è¿‘ 0ï¼Œé€™é€šå¸¸æ˜¯æ•¸æ“šéŒ¯èª¤æˆ– LLM Hallucinationï¼Œä¸æ˜¯çœŸæ­£çš„ç„¡é¢¨éšªå¥—åˆ©
    data_quality_issue = False
    if weighted_downside < 0.001:
        # æª¢æŸ¥æ˜¯å¦æ˜¯åˆç†çš„ã€Œç„¡é¢¨éšªã€æƒ…å¢ƒï¼ˆä¾‹å¦‚åœ‹å‚µã€è²¨å¹£åŸºé‡‘ï¼‰
        # å¦‚æœä¸æ˜¯ï¼Œé€™æ˜¯æ•¸æ“šéŒ¯èª¤
        if abs(r_bear) < 0.01 and abs(r_base) < 0.01:
            # Bear å’Œ Base éƒ½æ¥è¿‘ 0ï¼Œé€™ä¸åˆç†ï¼ˆé™¤éæ˜¯ç¾é‡‘ç­‰åƒ¹ç‰©ï¼‰
            data_quality_issue = True
            # å¼·åˆ¶è¨­å®šä¸€å€‹æœ€å°é¢¨éšªï¼Œé¿å…é™¤ä»¥é›¶
            weighted_downside = 0.05  # å‡è¨­è‡³å°‘æœ‰ 5% çš„æ½›åœ¨è™§æ
        else:
            # çœŸæ­£çš„ä½é¢¨éšªæƒ…å¢ƒï¼ˆä¾‹å¦‚ p_bear å¾ˆä½ï¼‰
            rr_ratio = 10.0  # ä¿ç•™åŸé‚è¼¯
    else:
        rr_ratio = weighted_upside / weighted_downside

    # --- 5. æœ€çµ‚åˆ¤æ±º (ç°¡å–®æ˜ç­) ---

    direction = "NEUTRAL"
    bias = "FLAT"
    conviction = 50

    # ğŸš¨ æ•¸æ“šè³ªé‡è¦†è“‹ (Data Quality Override)
    if data_quality_issue:
        direction = "NEUTRAL"
        bias = "UNCERTAIN"
        conviction = 30  # ä½ä¿¡å¿ƒ
        logger.warning(
            f"âš ï¸ Data Quality Issue detected for {ticker}: "
            f"Near-zero downside (r_bear={r_bear:.4f}, r_base={r_base:.4f}). "
            f"Forcing NEUTRAL verdict."
        )
    else:
        # æ¢ä»¶ A: é¡¯è‘—çœ‹å¤š
        # è³ ç‡ > 2.0 (è³ºçš„æ½›åŠ›æ˜¯è³ çš„å…©å€) ä¸” Alpha æ˜¯æ­£çš„
        if rr_ratio > 2.0 and alpha > 0:
            direction = "STRONG_LONG"
            bias = "BULLISH"
            conviction = 90

        # æ¢ä»¶ B: æ™®é€šçœ‹å¤š
        # è³ ç‡ > 1.3 (ç¨å¾®åˆ’ç®—) ä¸” Alpha æ˜¯æ­£çš„
        elif rr_ratio > 1.3 and alpha > 0:
            direction = "LONG"
            bias = "BULLISH"
            conviction = 70

        # æ¢ä»¶ C: å¿…é ˆåšç©º (åƒåœ¾è‚¡)
        # æœŸæœ›å€¼è·‘è¼¸ç¾å‚µï¼Œä¸” è³ ç‡å¾ˆå·® (è³ºçš„æ½›åŠ› < è³ çš„é¢¨éšª)
        elif alpha < 0 and rr_ratio < 0.8:
            direction = "SHORT"
            bias = "BEARISH"
            conviction = 70

        # æ¢ä»¶ D: é›è‚‹ / è§€æœ›
        else:
            # å¦‚æœ Alpha æ˜¯è² çš„ï¼Œä½†è³ ç‡é‚„å¯ä»¥ (rr_ratio > 1)ï¼Œèªªæ˜æ˜¯ "é£Ÿä¹‹ç„¡å‘³æ£„ä¹‹è€ƒæ…®"
            if alpha < 0:
                direction = "AVOID"  # å»ºè­°åˆ¥è²·ï¼Œä½†ä¹Ÿåˆ¥ç©º
                bias = "BEARISH"
            else:
                direction = "NEUTRAL"  # çœŸçš„æ²’æ–¹å‘
                bias = "FLAT"

    return {
        "ticker": ticker,
        "final_verdict": direction,
        "analysis_bias": bias,
        "rr_ratio": round(rr_ratio, 2),  # é€™æ˜¯æœ€ç›´è§€çš„æŒ‡æ¨™
        "alpha": round(alpha, 4),
        "raw_ev": round(raw_ev, 4),
        "conviction": conviction,
        "model_summary": f"Reward/Risk: {rr_ratio:.2f}x, Alpha: {alpha:.2%}",
        "risk_free_benchmark": round(risk_free, 4),
        "data_quality_warning": data_quality_issue,  # æ–°å¢ï¼šæ•¸æ“šè³ªé‡è­¦å‘Š
    }


def compress_financial_data(financial_reports: list[dict]) -> list[dict]:
    """
    Compresses raw SEC financial reports by removing metadata and flattening the structure.
    Optimizes for LLM context window.
    """
    compressed = []
    for report in financial_reports:
        # Extract basic info
        base = report.get("base") or {}
        ext = report.get("extension") or {}

        # We care about the year and the numerical values
        year = base.get("fiscal_year", {}).get("value", "Unknown")

        # Flattened metrics
        metrics = {}

        # Helper to extract value safely
        def get_val(item):
            if isinstance(item, dict):
                return item.get("value")
            return item

        # Base metrics
        main_fields = [
            "total_revenue",
            "net_income",
            "operating_cash_flow",
            "total_assets",
            "total_liabilities",
            "total_equity",
            "cash_and_equivalents",
            "shares_outstanding",
        ]
        for field in main_fields:
            val = get_val(base.get(field))
            if val is not None:
                metrics[field] = val

        # Extension metrics
        ext_fields = [
            "inventory",
            "accounts_receivable",
            "cogs",
            "rd_expense",
            "sga_expense",
            "capex",
        ]
        for field in ext_fields:
            val = get_val(ext.get(field))
            if val is not None:
                metrics[field] = val

        compressed.append(
            {
                "fiscal_year": year,
                "metrics": metrics,
                "industry": report.get("industry_type", "Unknown"),
            }
        )

    return compressed


def compress_news_data(news_output: dict) -> list[dict]:
    """
    Compresses news research output by removing full content and technical metadata.
    """
    if not news_output or "news_items" not in news_output:
        return []

    compressed = []
    for item in news_output.get("news_items", []):
        analysis = item.get("analysis") or {}

        # Focus on the summary and key facts
        compressed_item = {
            "date": item.get("published_at", "N/A")[:10],  # Just YYYY-MM-DD
            "title": item.get("title"),
            "source": item.get("source", {}).get("name", "Unknown"),
            "summary": analysis.get("summary"),
            "sentiment": analysis.get("sentiment"),
            "impact": analysis.get("impact_level"),
            "key_facts": [f.get("content") for f in analysis.get("key_facts", [])],
        }
        compressed.append(compressed_item)

    return compressed


def compress_ta_data(ta_output: dict | None) -> dict | None:
    """
    Compresses technical analysis output for debate context.
    Focuses on semantic tags and key metrics, removes raw data.
    """
    if not ta_output:
        return None

    # Extract key information
    compressed = {
        "ticker": ta_output.get("ticker"),
        "timestamp": ta_output.get("timestamp"),
        "signal_summary": {
            "z_score": ta_output.get("signal_state", {}).get("z_score"),
            "direction": ta_output.get("signal_state", {}).get("direction"),
            "risk_level": ta_output.get("signal_state", {}).get("risk_level"),
            "statistical_state": ta_output.get("signal_state", {}).get(
                "statistical_state"
            ),
        },
        "memory_metrics": {
            "optimal_d": ta_output.get("frac_diff_metrics", {}).get("optimal_d"),
            "memory_strength": ta_output.get("frac_diff_metrics", {}).get(
                "memory_strength"
            ),
        },
        "semantic_tags": ta_output.get("semantic_tags", []),
        "interpretation": ta_output.get("llm_interpretation"),
    }

    return compressed
